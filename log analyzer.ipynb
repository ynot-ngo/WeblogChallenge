{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp                   1158500\n",
      "elb                         1158500\n",
      "client:port                 1158500\n",
      "backend:port                1158500\n",
      "request_processing_time     1158500\n",
      "backend_processing_time     1158500\n",
      "response_processing_time    1158500\n",
      "elb_status_code             1158500\n",
      "backend_status_code         1158500\n",
      "received_bytes              1158500\n",
      "sent_bytes                  1158500\n",
      "request                     1158500\n",
      "user_agent                  1158495\n",
      "ssl_cipher                  1158500\n",
      "ssl_protocol                1158500\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "spark = SparkSession.builder.appName('LogAnalyzer').getOrCreate()\n",
    "\n",
    "# Set column names\n",
    "col_names = [\"timestamp\", \"elb\", \"client:port\", \"backend:port\", \"request_processing_time\", \"backend_processing_time\",\n",
    "                \"response_processing_time\", \"elb_status_code\", \"backend_status_code\", \"received_bytes\", \"sent_bytes\", \n",
    "                \"request\", \"user_agent\", \"ssl_cipher\", \"ssl_protocol\"]\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv('data/2015_07_22_mktplace_shop_web_log_sample.log.gz', delim_whitespace=True, names=col_names, compression='gzip')\n",
    "\n",
    "# Ensure record count is consistent to file\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Create df of features needed for analysis\n",
    "df_sessions = df.loc[:,[ \"client:port\", \"request\", \"timestamp\"]]\n",
    "\n",
    "# Strip out port from client:port to create \"unique user IP\" \n",
    "df_sessions[\"client_ip\"] = df_sessions[\"client:port\"].str.split(\":\").str[0]\n",
    "df_sessions.drop(\"client:port\", axis=1, inplace=True)\n",
    "\n",
    "# Format timestamp from ISO-8601 string to datetime format \n",
    "df_sessions[\"timestamp\"] = pd.to_datetime(df_sessions[\"timestamp\"])\n",
    "\n",
    "# Sort data (required later)\n",
    "df_sessions = df_sessions.sort_values(by=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Prepare schema for temp table\n",
    "fields = [StructField(\"request\", StringType(), True), StructField(\"timestamp\", TimestampType(), True), StructField(\"client_ip\", StringType(), True) ]\n",
    "schema = StructType(fields)\n",
    "# Apply schema\n",
    "sparkDfSessions = spark.createDataFrame(df_sessions, schema)\n",
    "sparkDfSessions.registerTempTable(\"weblogSessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+\n",
      "|             request|           timestamp|      client_ip|\n",
      "+--------------------+--------------------+---------------+\n",
      "|GET https://paytm...|2015-07-21 22:40:...| 106.51.235.133|\n",
      "|GET https://paytm...|2015-07-21 22:40:...| 115.250.16.146|\n",
      "|GET https://paytm...|2015-07-21 22:40:...| 106.51.235.133|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|   52.74.219.71|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|  27.97.124.172|\n",
      "|GET https://paytm...|2015-07-21 22:40:...| 106.78.125.179|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|   112.79.36.98|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|  119.81.61.166|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|117.197.179.139|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|    1.39.14.113|\n",
      "|POST https://payt...|2015-07-21 22:40:...| 49.206.246.124|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|  119.81.61.166|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|  119.81.61.166|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|   74.125.63.33|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|  119.81.61.166|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|  15.211.153.78|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|  119.81.61.166|\n",
      "|GET https://paytm...|2015-07-21 22:40:...| 122.172.38.214|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|  119.81.61.166|\n",
      "|GET https://paytm...|2015-07-21 22:40:...|  49.156.68.161|\n",
      "+--------------------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM weblogSessions').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------+--------------------------+\n",
      "|client_ip   |timestamp                 |previous_timestamp        |\n",
      "+------------+--------------------------+--------------------------+\n",
      "|1.186.143.37|2015-07-22 12:14:36.308131|null                      |\n",
      "|1.186.143.37|2015-07-22 12:14:44.846873|2015-07-22 12:14:36.308131|\n",
      "|1.187.164.29|2015-07-21 22:43:31.135201|null                      |\n",
      "|1.187.164.29|2015-07-21 22:43:42.735304|2015-07-21 22:43:31.135201|\n",
      "|1.187.164.29|2015-07-21 22:43:47.685225|2015-07-21 22:43:42.735304|\n",
      "|1.187.164.29|2015-07-21 22:44:19.075268|2015-07-21 22:43:47.685225|\n",
      "|1.187.164.29|2015-07-21 22:44:20.084871|2015-07-21 22:44:19.075268|\n",
      "|1.187.164.29|2015-07-21 22:44:21.995557|2015-07-21 22:44:20.084871|\n",
      "|1.187.164.29|2015-07-21 22:44:23.254646|2015-07-21 22:44:21.995557|\n",
      "|1.187.164.29|2015-07-21 22:44:23.295011|2015-07-21 22:44:23.254646|\n",
      "|1.187.164.29|2015-07-21 22:44:40.245266|2015-07-21 22:44:23.295011|\n",
      "|1.22.41.76  |2015-07-22 12:42:59.01417 |null                      |\n",
      "|1.22.41.76  |2015-07-22 12:43:07.155189|2015-07-22 12:42:59.01417 |\n",
      "|1.22.41.76  |2015-07-22 12:43:29.15416 |2015-07-22 12:43:07.155189|\n",
      "|1.22.41.76  |2015-07-22 12:43:31.846037|2015-07-22 12:43:29.15416 |\n",
      "|1.22.41.76  |2015-07-22 12:43:55.687425|2015-07-22 12:43:31.846037|\n",
      "|1.22.41.76  |2015-07-22 12:43:57.475919|2015-07-22 12:43:55.687425|\n",
      "|1.22.41.76  |2015-07-22 12:43:59.375379|2015-07-22 12:43:57.475919|\n",
      "|1.22.41.76  |2015-07-22 12:44:01.322622|2015-07-22 12:43:59.375379|\n",
      "|1.23.208.26 |2015-07-22 13:41:41.431398|null                      |\n",
      "+------------+--------------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine the previous timestamp entry by using LAG window func as \n",
    "# data grouped by client_ip as timestamp in ascending order\n",
    "\n",
    "weblogSessions_timestamps_df = spark.sql(\"\"\"\n",
    "    SELECT client_ip, request, timestamp, \n",
    "        LAG(timestamp) OVER (PARTITION BY client_ip ORDER BY timestamp) AS previous_timestamp\n",
    "    FROM weblogSessions\"\"\")\n",
    "\n",
    "weblogSessions_timestamps_df.registerTempTable(\"weblogSessions_timestamps\")\n",
    "spark.sql('select client_ip, timestamp, previous_timestamp from weblogSessions_timestamps').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Sessionize the web log by IP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated using current - previous timestamp within unique client_ip \n",
    "# and continuous sesion distinguished via session threshold\n",
    "# using 15 mins as session interval (can try standard 30 mins intervals also)\n",
    "\n",
    "weblogSessions_sessions_df = spark.sql(\"\"\"    \n",
    "    SELECT *,\n",
    "        CASE \n",
    "            WHEN unix_timestamp(timestamp) - unix_timestamp(previous_timestamp) >= (60 * 15) \n",
    "            OR previous_timestamp IS NULL\n",
    "        THEN 1 ELSE 0 END AS is_new_session\n",
    "    FROM weblogSessions_timestamps\"\"\")\n",
    "weblogSessions_sessions_df.registerTempTable(\"weblogSessions_sessions\")\n",
    "# spark.sql(\"select client_ip, timestamp, previous_timestamp, is_new_session from weblogSessions_sessions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-18d5a4cb41aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create id per sessions (running total of is_new_session)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m weblogSessions_sessionIds_df = spark.sql(\"\"\"    \n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mSELECT\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSUM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_new_session\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mOVER\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPARTITION\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mclient_ip\u001b[0m \u001b[0mORDER\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mAS\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mFROM\u001b[0m \u001b[0mweblogSessions_sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Create id per sessions (running total of is_new_session)\n",
    "\n",
    "weblogSessions_sessionIds_df = spark.sql(\"\"\"    \n",
    "    SELECT *, SUM(is_new_session) OVER (PARTITION BY client_ip ORDER BY timestamp) AS session_id\n",
    "    FROM weblogSessions_sessions\n",
    "    \"\"\")\n",
    "weblogSessions_sessionIds_df.registerTempTable(\"weblogSessions_sessionIds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Determine the average session time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------+----------------------------+\n",
      "|amount_of_time_in_seconds|num_of_sessions|average_session_time_in_mins|\n",
      "+-------------------------+---------------+----------------------------+\n",
      "|                 11161579|         110841|          1.6783168382337463|\n",
      "+-------------------------+---------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total session time across all sessions (based on client_ip and session_id)\n",
    "# Sum the total number of sessions (with threshold 15 mins) per client_ip\n",
    "# Divide total session time by total number of sessions for avg session time.\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    SUM(total_session_time_per_ip) AS amount_of_time_in_seconds, \n",
    "    SUM(num_of_sessions_per_ip) AS num_of_sessions,\n",
    "    (SUM(total_session_time_per_ip) / SUM(num_of_sessions_per_ip)) / 60 AS average_session_time_in_mins\n",
    "FROM (\n",
    "    SELECT client_ip, MAX(session_id) AS num_of_sessions_per_ip, SUM(session_time) AS total_session_time_per_ip\n",
    "    FROM (\n",
    "        SELECT client_ip, session_id, MAX(unix_timestamp(timestamp)) - MIN(unix_timestamp(timestamp)) AS session_time\n",
    "        FROM weblogSessions_sessionIds\n",
    "        GROUP BY client_ip, session_id \n",
    "        ORDER BY client_ip, session_id\n",
    "    ) grouped_sessions\n",
    "    GROUP BY client_ip\n",
    ") average_sessions\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Determine unique URL visits per session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------+-------------------+-------------------------+\n",
      "|   client_ip|total_sessions_per_ip|total_unique_visits|unique_visits_per_session|\n",
      "+------------+---------------------+-------------------+-------------------------+\n",
      "|1.186.143.37|                    1|                  2|                      2.0|\n",
      "|1.187.164.29|                    1|                  8|                      8.0|\n",
      "|  1.22.41.76|                    1|                  5|                      5.0|\n",
      "| 1.23.208.26|                    2|                  6|                      3.0|\n",
      "| 1.23.36.184|                    1|                  4|                      4.0|\n",
      "|   1.38.19.8|                    1|                  1|                      1.0|\n",
      "|  1.38.20.34|                    1|                 14|                     14.0|\n",
      "|  1.39.13.13|                    1|                  2|                      2.0|\n",
      "| 1.39.32.249|                    2|                  6|                      3.0|\n",
      "|  1.39.32.59|                    1|                  1|                      1.0|\n",
      "| 1.39.33.153|                    1|                  6|                      6.0|\n",
      "|  1.39.33.33|                    1|                  2|                      2.0|\n",
      "|  1.39.33.77|                    2|                  6|                      3.0|\n",
      "|   1.39.34.4|                    1|                  1|                      1.0|\n",
      "|  1.39.40.43|                    1|                  2|                      2.0|\n",
      "|  1.39.60.37|                    1|                 36|                     36.0|\n",
      "|  1.39.61.53|                    1|                 19|                     19.0|\n",
      "| 1.39.62.227|                    1|                  2|                      2.0|\n",
      "| 1.39.63.197|                    1|                  2|                      2.0|\n",
      "|   1.39.63.5|                    2|                  6|                      3.0|\n",
      "+------------+---------------------+-------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine distinct URL requests by clients per session.\n",
    "# Number of unique requests per client divided by the total number of sessions per client is unique URL per session\n",
    "\n",
    "spark.sql(\"\"\"    \n",
    "    SELECT\n",
    "        client_ip, MAX(session_id) AS total_sessions_per_ip, COUNT(request) AS total_unique_visits,\n",
    "        COUNT(request) / MAX(session_id) AS unique_visits_per_session\n",
    "    FROM (\n",
    "        SELECT DISTINCT client_ip, session_id, request\n",
    "        FROM weblogSessions_sessionIds  \n",
    "    ) sessions\n",
    "    GROUP BY client_ip\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Find the most engaged users, ie the IPs with the longest session times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------------+\n",
      "|      client_ip|session_id|session_time_in_mins|\n",
      "+---------------+----------+--------------------+\n",
      "|  119.81.61.166|         5|  34.483333333333334|\n",
      "|   52.74.219.71|         5|  34.483333333333334|\n",
      "|  106.186.23.95|         5|  34.483333333333334|\n",
      "|   125.20.39.66|         4|   34.46666666666667|\n",
      "|   125.19.44.66|         5|   34.46666666666667|\n",
      "| 180.211.69.209|         4|               34.45|\n",
      "|   192.8.190.10|         3|               34.45|\n",
      "|  54.251.151.39|         5|               34.45|\n",
      "| 180.179.213.70|         5|   34.43333333333333|\n",
      "|  122.15.156.64|         3|   34.43333333333333|\n",
      "| 203.191.34.178|         2|   34.43333333333333|\n",
      "| 203.189.176.14|         5|   34.43333333333333|\n",
      "| 180.151.80.140|         4|  34.416666666666664|\n",
      "| 125.16.218.194|         1|  34.416666666666664|\n",
      "| 103.29.159.138|         1|  34.416666666666664|\n",
      "|213.239.204.204|         2|  34.416666666666664|\n",
      "|    78.46.60.71|         1|                34.4|\n",
      "| 103.29.159.186|         2|                34.4|\n",
      "|  192.71.175.30|         3|   34.38333333333333|\n",
      "|   14.99.226.79|         1|   34.38333333333333|\n",
      "+---------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by client_ip and their sessions, and calculate the longest session they held\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT client_ip, session_id, (MAX(unix_timestamp(timestamp)) - MIN(unix_timestamp(timestamp))) / 60 AS session_time_in_mins\n",
    "    FROM weblogSessions_sessionIds\n",
    "    GROUP BY client_ip, session_id\n",
    "    ORDER BY session_time_in_mins DESC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Notes:\n",
    "\n",
    "IP addresses do not guarantee distinct users, but this is the limitation of the data. As a bonus, consider what additional data would help make better analytical conclusions.\n",
    "\n",
    "There are additional methods for identifying visitors, sessions and paths:\n",
    "\n",
    "1. IP-Agent, in addition to IP address you can use user-agent (browser) information to determine unique sessions. This will help detect multiple users from one IP address. \n",
    "\n",
    "2. Username data if the website is configured to require authentication.\n",
    "\n",
    "3. Session ID, if found in URL query or stored in a Cookie can result in more accurate visitor session and path analysis.\n",
    "\n",
    "In addition, we can also filter entries which do not add value to the analytics. \n",
    "Requests that are made by automatic activity such as bots can be filtered.\n",
    "Or some requests for components of the webpage such as to images may not be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
